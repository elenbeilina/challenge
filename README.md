# Introduction

In order to achieve integration between our systems with do use Kafka connectors.

Connectors allow copying data between Kafka and other systems that you want to pull data from or push data to.

We distinguish between two kinds of connectors, Source connector and sink connector.

- Source connector allow copying data from external system to kafka topic.
- Sink connector allow taking data from Kafka topic to external system.

In order to integrate Kafka with other system we can use existing connector implementations for common data sources and sinks,
or implement our own connectors.

Your task will be to create two connectors, the first one will source the data to 
a specific topic, and the second one will allow sinking the data to external API.

# Tasks:
### 1. Random String Source Connector
 
 - The responsibility of this connector is to generate some random data and pushes the results to a specific topic.
 - The name of the topic should be provided to the connector during the configuration.
 - The random data can be generated via Faker lib:
 
 ```
 Faker faker = new Faker();
 faker.chuckNorris().fact()
 ```

 For this connector you will need to write the following:
    
 - Connector Class: start and taskConfigs methods need to be implemented.
 - Config Class: Implement the property that will allow to pass the topic name.
 - Task Class: poll method needs to be implemented and each record will be generated should have a structure.
 - Cover you code with unit testing the implemented methods.
 
```
{
  "namespace": "org.app.connector",
  "name": "ItemValue",
  "type": "record"
  "fields": [
    {
      "name": "value",
      "type": "string"
    }
  ],
}
```
    
### 2. Rest Sink Connector

- The responsibility of this connector is to read data that has been generated by the first connector and send it via Rest Call to an API.
- The name of the topic and the API URI should be provided to the connector during the configuration.
- Each record read will be sent to the external API via Http POST Request.
```
To mock the API, you can use this service: https://pipedream.com/
```
 For this connector you will need to write the following:

- Connector Class: start and taskConfigs methods need to be implemented.
- Config Class: Implement the necessary property for API URI.
- Task Class: put method needs to be implemented and each record will need to be sent the external API via an HTTP Post Request.
- Cover you code with unit testing for the implemented methods

### Testing:

To test you code we did provide a docker-compose with the necessary element to run a single node cluster with kafka connect
A `docker-compose.yml` in the root of the folder contains the following:
- kafka
- zookeeper
- schema-register
- kafka-connect
- control-center


To build the challenge project:
The project contains the necessary dependency:
- Apache Connect API
- Faker
- OkHttp

```
cd challenge
mvn clean package
```

**To run you environment**
```
docker-compose up -d
```

**To inspect running containers**
```
docker-compose ps
```

**To run the source connector**
```
./source.sh
```

**To run the sink connector**
```
./sink.sh
```

**To check your environment via the browser**
```
http://localhost:9021/clusters
```

### Results

Once you are done, Please make sure to share you code either via zip file or publish it to public repository and share it
with us, as well explain all steps that are missing in the implementation.
If needed please explain the instructions to follow to run you code.

Thanks!